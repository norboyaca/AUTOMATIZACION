/**
 * ===========================================
 * SERVICIO DE EMBEDDINGS - OPENAI
 * ===========================================
 *
 * B√∫squeda vectorial ultra optimizada para recuperaci√≥n de chunks.
 *
 * OPTIMIZACIONES:
 * - text-embedding-3-small (m√°s barato y r√°pido)
 * - Batch processing (100 chunks por API call)
 * - Cach√© en memoria (evita regeneraci√≥n)
 * - Retry logic con exponential backoff
 * - Rate limiting autom√°tico
 *
 * COSTOS:
 * - Generaci√≥n: $0.00002 por 1K tokens
 * - 494 chunks √ó ~100 tokens = ~$0.001 (√∫nica vez)
 * - Por consulta: 1 embedding √ó ~10 tokens = $0.0000002
 */

const OpenAI = require('openai');
const logger = require('../utils/logger');
const settingsService = require('./settings.service');

// ===========================================
// CONFIGURACI√ìN
// ===========================================

const EMBEDDING_CONFIG = {
  model: process.env.EMBEDDING_MODEL || 'text-embedding-3-small',
  batchSize: parseInt(process.env.EMBEDDING_BATCH_SIZE) || 100,
  maxRetries: 3,
  retryDelay: 1000, // ms
  retryBackoff: 2, // multiplicador
};

// Cliente OpenAI (singleton)
let openaiClient = null;

// ===========================================
// CACHE EN MEMORIA
// ===========================================

/**
 * ‚úÖ OPTIMIZADO: Cache de embeddings de QUERIES para evitar API calls repetidos
 * Estructura: { normalizedQuery: { embedding, timestamp } }
 * TTL: 5 minutos
 */
const queryEmbeddingCache = new Map();
const QUERY_CACHE_TTL = 5 * 60 * 1000; // 5 minutos
const QUERY_CACHE_MAX_SIZE = 200; // M√°ximo queries cacheadas

/**
 * Chunks cargados en memoria
 * Estructura: [ { id, text, embedding, ...chunkData } ]
 */
let loadedChunks = [];

/**
 * Flag para saber si los chunks est√°n cargados
 */
let chunksLoaded = false;

/**
 * ‚úÖ OPTIMIZADO: Cache del proveedor de embeddings detectado
 * Se invalida cuando cambian los settings
 */
let cachedProvider = null;
let cachedProviderTimestamp = 0;
const PROVIDER_CACHE_TTL = 60 * 1000; // Re-evaluar cada 60 segundos

// ===========================================
// INICIALIZACI√ìN
// ===========================================

/**
 * Inicializa el cliente de OpenAI
 */
function initializeClient() {
  if (!openaiClient) {
    const apiKey = process.env.OPENAI_API_KEY;

    if (!apiKey) {
      throw new Error('OPENAI_API_KEY no est√° definida en variables de entorno');
    }

    openaiClient = new OpenAI({
      apiKey: apiKey,
      timeout: 30000, // 30 segundos
      maxRetries: EMBEDDING_CONFIG.maxRetries,
    });

    logger.info('‚úÖ Cliente OpenAI embeddings inicializado');
  }

  return openaiClient;
}

/**
 * ‚úÖ CRITICAL FIX: Detecta qu√© proveedor de embeddings usar
 *
 * PRIORIDAD #1: Debe coincidir con las dimensiones de los embeddings almacenados.
 * Si los chunks tienen embeddings de 384 dims (Xenova), las queries DEBEN usar Xenova.
 * Si los chunks tienen embeddings de 1536 dims (OpenAI), las queries DEBEN usar OpenAI.
 * Mezclar proveedores causa que cosine similarity retorne 0 (dimensiones ‚â†).
 *
 * @returns {string} 'openai' o 'xenova'
 */
function detectEmbeddingProvider() {
  const now = Date.now();

  // Usar cache si no ha expirado
  if (cachedProvider && (now - cachedProviderTimestamp) < PROVIDER_CACHE_TTL) {
    return cachedProvider;
  }

  // ‚úÖ CRITICAL: Auto-detectar desde embeddings almacenados
  // Si ya hay chunks cargados, usar el MISMO proveedor que gener√≥ esos embeddings
  if (loadedChunks.length > 0) {
    const sampleChunk = loadedChunks.find(c => c.embedding && c.embedding.length > 0);
    if (sampleChunk) {
      const dims = sampleChunk.embedding.length;
      if (dims === 384) {
        // Xenova/all-MiniLM-L6-v2 usa 384 dimensiones
        cachedProvider = 'xenova';
        cachedProviderTimestamp = now;
        logger.info(`üîç Proveedor detectado desde chunks almacenados: xenova (${dims} dims)`);
        return cachedProvider;
      } else if (dims === 1536) {
        // OpenAI text-embedding-3-small usa 1536 dimensiones
        cachedProvider = 'openai';
        cachedProviderTimestamp = now;
        logger.info(`üîç Proveedor detectado desde chunks almacenados: openai (${dims} dims)`);
        return cachedProvider;
      }
      // Si dimensiones no coinciden con ning√∫n proveedor conocido, seguir con la l√≥gica normal
      logger.warn(`‚ö†Ô∏è Dimensiones de embedding desconocidas: ${dims}. Usando l√≥gica por defecto.`);
    }
  }

  // Fallback: l√≥gica basada en settings (solo para NUEVOS embeddings sin chunks previos)
  const settings = settingsService.getApiKeys();
  const chatGPTEnabled = settings.openai.enabled && settings.openai.apiKey;
  const grokEnabled = settings.groq.enabled && settings.groq.apiKey;

  if (chatGPTEnabled && !grokEnabled) {
    cachedProvider = 'openai';
  } else {
    cachedProvider = 'xenova';
  }

  cachedProviderTimestamp = now;
  logger.info(`üîç Proveedor de embeddings (por settings): ${cachedProvider}`);
  return cachedProvider;
}

// ===========================================
// GENERACI√ìN DE EMBEDDINGS
// ===========================================

/**
 * Genera embeddings para un batch de textos
 *
 * @param {Array<string>} texts - Textos para generar embeddings
 * @returns {Promise<Array<number[]>>} Array de embeddings
 */
async function generateEmbeddingsBatch(texts) {
  const client = initializeClient();

  logger.debug(`üîÑ Generando embeddings para ${texts.length} textos...`);

  try {
    const response = await client.embeddings.create({
      model: EMBEDDING_CONFIG.model,
      input: texts,
      encoding_format: 'float',
    });

    const embeddings = response.data.map(item => item.embedding);

    logger.debug(`‚úÖ Embeddings generados: ${embeddings.length}`);

    return embeddings;
  } catch (error) {
    logger.error(`‚ùå Error generando embeddings: ${error.message}`);

    // Retry logic con exponential backoff
    if (error.status === 429 || error.status >= 500) {
      logger.warn(`‚ö†Ô∏è Rate limit o error del servidor, reintentando...`);

      for (let attempt = 1; attempt <= EMBEDDING_CONFIG.maxRetries; attempt++) {
        const delay = EMBEDDING_CONFIG.retryDelay * Math.pow(EMBEDDING_CONFIG.retryBackoff, attempt - 1);

        logger.info(`üîÑ Reintento ${attempt}/${EMBEDDING_CONFIG.maxRetries} en ${delay}ms...`);

        await sleep(delay);

        try {
          const response = await client.embeddings.create({
            model: EMBEDDING_CONFIG.model,
            input: texts,
          });

          const embeddings = response.data.map(item => item.embedding);

          logger.info(`‚úÖ Embeddings generados en reintento ${attempt}`);

          return embeddings;
        } catch (retryError) {
          logger.warn(`‚ùå Reintento ${attempt} fall√≥: ${retryError.message}`);

          if (attempt === EMBEDDING_CONFIG.maxRetries) {
            throw new Error(`Fallaron ${EMBEDDING_CONFIG.maxRetries} reintentos: ${retryError.message}`);
          }
        }
      }
    }

    throw error;
  }
}

// ===========================================
// ‚úÖ NUEVO: GENERACI√ìN DE EMBEDDINGS LOCALES
// ===========================================

/**
 * Cache para el pipeline de Xenova (singleton)
 */
let xenovaPipeline = null;

/**
 * Genera embeddings locales usando @xenova/transformers
 * Modelo: Xenova/all-MiniLM-L6-v2 (384 dimensiones)
 *
 * @param {Array<string>} texts - Textos para generar embeddings
 * @returns {Promise<Array<number[]>>} Array de embeddings
 */
async function generateEmbeddingsLocal(texts) {
  const { pipeline } = require('@xenova/transformers');

  // Inicializar pipeline solo una vez (lazy loading)
  if (!xenovaPipeline) {
    logger.info('üß† Inicializando modelo de embeddings local (@xenova/transformers)...');
    logger.info('   Modelo: Xenova/all-MiniLM-L6-v2');
    logger.info('   Dimensiones: 384');

    try {
      xenovaPipeline = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', {
        progress_callback: (progress) => {
          if (progress.status === 'downloading') {
            const percentage = progress.progress || 0;
            logger.debug(`   üì• Descargando modelo: ${percentage.toFixed(1)}%`);
          } else if (progress.status === 'loading') {
            logger.debug(`   üì¶ Cargando modelo en memoria...`);
          }
        }
      });

      logger.info('‚úÖ Modelo de embeddings local inicializado correctamente');
    } catch (error) {
      logger.error('‚ùå Error inicializando modelo local:', error.message);
      throw new Error(`No se pudo inicializar el modelo de embeddings local: ${error.message}`);
    }
  }

  logger.debug(`üîÑ Generando embeddings locales para ${texts.length} textos...`);

  try {
    const embeddings = [];

    // Generar embeddings uno por uno (m√°s estable que batch)
    for (let i = 0; i < texts.length; i++) {
      const text = texts[i];

      // Generar embedding usando el pipeline
      const output = await xenovaPipeline(text, {
        pooling: 'mean',
        normalize: true
      });

      // Convertir Tensor a array normal
      const embedding = Array.from(output.data);
      embeddings.push(embedding);

      // Log de progreso cada 10 textos
      if ((i + 1) % 10 === 0 || (i + 1) === texts.length) {
        logger.debug(`   Progreso: ${i + 1}/${texts.length} textos procesados`);
      }
    }

    logger.debug(`‚úÖ Embeddings locales generados: ${embeddings.length}`);
    return embeddings;
  } catch (error) {
    logger.error(`‚ùå Error generando embeddings locales: ${error.message}`);
    throw error;
  }
}

/**
 * Asegura que todos los chunks tengan embeddings
 * NO regenera si ya existen
 *
 * @param {Array} chunks - Array de chunks { text, embeddingGenerated?, embedding? }
 * @returns {Promise<Array>} Chunks con embeddings
 */
async function ensureEmbeddings(chunks) {
  if (!chunks || chunks.length === 0) {
    return [];
  }

  // Filtrar chunks que necesitan embeddings
  const chunksWithoutEmbeddings = chunks.filter(c => !c.embeddingGenerated && !c.embedding);

  if (chunksWithoutEmbeddings.length === 0) {
    logger.info('‚úÖ Todos los chunks ya tienen embeddings');
    return chunks;
  }

  logger.info(`üîÑ Generando embeddings para ${chunksWithoutEmbeddings.length} chunks...`);

  try {
    // ‚úÖ NUEVO: Detectar proveedor a usar
    const provider = detectEmbeddingProvider();
    logger.info(`ü§ñ Proveedor de embeddings: ${provider.toUpperCase()}`);

    // Tama√±o de batch seg√∫n proveedor
    // OpenAI: m√°s r√°pido, batches grandes (100)
    // Xenova: m√°s lento, batches peque√±os (10)
    const batchSize = provider === 'openai' ? EMBEDDING_CONFIG.batchSize : 10;

    // Procesar en batches
    const batches = chunkArray(chunksWithoutEmbeddings, batchSize);

    for (let i = 0; i < batches.length; i++) {
      const batch = batches[i];

      logger.info(`üì¶ Procesando batch ${i + 1}/${batches.length} (${batch.length} chunks)...`);

      const texts = batch.map(c => c.text || c);

      // ‚úÖ NUEVO: Usar proveedor detectado
      let embeddings;
      if (provider === 'openai') {
        embeddings = await generateEmbeddingsBatch(texts);
      } else {
        embeddings = await generateEmbeddingsLocal(texts);
      }

      // Asignar embeddings a chunks
      batch.forEach((chunk, j) => {
        chunk.embedding = embeddings[j];
        chunk.embeddingGenerated = true;
        chunk.embeddingDate = new Date().toISOString();
        chunk.embeddingProvider = provider; // ‚úÖ NUEVO: Guardar qu√© proveedor se us√≥
      });

      // Rate limiting SOLO para OpenAI (Xenova no lo necesita)
      if (provider === 'openai' && i < batches.length - 1) {
        await sleep(200); // 200ms entre batches = 5 batches/segundo = 300/min (seguro)
      }
    }

    logger.info(`‚úÖ Embeddings generados para ${chunksWithoutEmbeddings.length} chunks (proveedor: ${provider})`);

    return chunks;
  } catch (error) {
    logger.error(`‚ùå Error generando embeddings: ${error.message}`);
    throw error;
  }
}

// ===========================================
// B√öSQUEDA VECTORIAL
// ===========================================

/**
 * Calcula similitud coseno entre dos vectores
 *
 * @param {Array<number>} vecA - Vector A
 * @param {Array<number>} vecB - Vector B
 * @returns {number} Similitud coseno (-1 a 1)
 */
function cosineSimilarity(vecA, vecB) {
  if (!vecA || !vecB || vecA.length !== vecB.length) {
    return 0;
  }

  let dotProduct = 0;
  let magA = 0;
  let magB = 0;

  for (let i = 0; i < vecA.length; i++) {
    dotProduct += vecA[i] * vecB[i];
    magA += vecA[i] * vecA[i];
    magB += vecB[i] * vecB[i];
  }

  magA = Math.sqrt(magA);
  magB = Math.sqrt(magB);

  if (magA === 0 || magB === 0) {
    return 0;
  }

  return dotProduct / (magA * magB);
}

/**
 * ‚úÖ OPTIMIZADO: Obtiene embedding de una query, usando cache si est√° disponible
 *
 * @param {string} query - Texto de la consulta
 * @returns {Promise<number[]>} Embedding vector
 */
async function getQueryEmbedding(query) {
  // Normalizar query para cache
  const cacheKey = query.toLowerCase().trim().replace(/[¬ø?!¬°.,;:]/g, '').replace(/\s+/g, ' ');

  // Verificar cache
  const cached = queryEmbeddingCache.get(cacheKey);
  if (cached && (Date.now() - cached.timestamp) < QUERY_CACHE_TTL) {
    logger.debug(`‚ö° Query embedding cache HIT: "${query.substring(0, 30)}..."`);
    return cached.embedding;
  }

  // Generar nuevo embedding
  const provider = detectEmbeddingProvider();
  let queryEmbeddings;

  const startTime = Date.now();

  if (provider === 'openai') {
    queryEmbeddings = await generateEmbeddingsBatch([query]);
  } else {
    queryEmbeddings = await generateEmbeddingsLocal([query]);
  }

  const elapsed = Date.now() - startTime;
  logger.info(`üîÑ Query embedding generado en ${elapsed}ms (proveedor: ${provider})`);

  const embedding = queryEmbeddings[0];

  // Guardar en cache
  if (queryEmbeddingCache.size >= QUERY_CACHE_MAX_SIZE) {
    // Eliminar la entrada m√°s antigua
    const oldestKey = queryEmbeddingCache.keys().next().value;
    queryEmbeddingCache.delete(oldestKey);
  }
  queryEmbeddingCache.set(cacheKey, { embedding, timestamp: Date.now() });

  return embedding;
}

/**
 * Busca los chunks m√°s relevantes usando embeddings
 *
 * ‚úÖ OPTIMIZADO: Usa cache de query embeddings y logging de tiempos
 *
 * @param {string} query - Pregunta del usuario
 * @param {number} limit - Cantidad de resultados (default: 15 para re-ranking)
 * @returns {Promise<Array>} Chunks m√°s relevantes con similitud
 */
async function findRelevantChunks(query, limit = 15) {
  const totalStart = Date.now();

  try {
    // Asegurar que los chunks est√©n cargados
    if (!chunksLoaded) {
      logger.warn('‚ö†Ô∏è Chunks no cargados, cargando...');
      await loadAllChunks();
    }

    if (loadedChunks.length === 0) {
      logger.warn('‚ö†Ô∏è No hay chunks cargados para b√∫squeda');
      return [];
    }

    // ‚úÖ OPTIMIZADO: Usar cache de query embeddings
    logger.debug(`üîç Obteniendo embedding para consulta: "${query.substring(0, 50)}..."`);
    const queryVector = await getQueryEmbedding(query);

    // Calcular similitud con todos los chunks (100% local, sin API)
    const searchStart = Date.now();
    logger.debug(`üìä Calculando similitud con ${loadedChunks.length} chunks...`);

    const results = loadedChunks
      .filter(chunk => chunk.embedding) // Solo chunks con embedding
      .map(chunk => ({
        ...chunk,
        similarity: cosineSimilarity(queryVector, chunk.embedding),
      }))
      .filter(result => result.similarity > 0) // Solo resultados con similitud positiva
      .sort((a, b) => b.similarity - a.similarity) // Ordenar por similitud descendente
      .slice(0, limit); // Top K

    const searchElapsed = Date.now() - searchStart;
    const totalElapsed = Date.now() - totalStart;

    logger.info(`‚úÖ Encontrados ${results.length} chunks relevantes (b√∫squeda: ${searchElapsed}ms, total: ${totalElapsed}ms)`);
    logger.debug(`üìä Top 3 similitudes: ${results.slice(0, 3).map(r => r.similarity.toFixed(4)).join(', ')}`);

    return results;
  } catch (error) {
    logger.error(`‚ùå Error en b√∫squeda vectorial: ${error.message}`);
    throw error;
  }
}

// ===========================================
// GESTI√ìN DE CHUNKS
// ===========================================

/**
 * Carga todos los chunks con embeddings desde archivos JSON
 */
async function loadAllChunks() {
  if (chunksLoaded) {
    logger.debug('‚úÖ Chunks ya est√°n cargados en memoria');
    return loadedChunks;
  }

  logger.info('üìÇ Cargando chunks con embeddings...');

  const knowledgeUploadService = require('./knowledge-upload.service');
  const files = knowledgeUploadService.getUploadedFiles();

  loadedChunks = [];

  for (const file of files) {
    try {
      const data = await knowledgeUploadService.getFileData(file);

      if (data && data.chunks) {
        // Agregar metadatos del archivo
        const chunksWithMeta = data.chunks.map(chunk => ({
          ...chunk,
          source: file.originalName,
          sourceId: file.id,
        }));

        loadedChunks.push(...chunksWithMeta);
      }
    } catch (error) {
      logger.warn(`‚ö†Ô∏è Error cargando datos de ${file.originalName}: ${error.message}`);
    }
  }

  // Filtrar solo chunks con embeddings
  const chunksWithEmbeddings = loadedChunks.filter(c => c.embedding && c.embedding.length > 0);
  const chunksWithoutEmbeddings = loadedChunks.length - chunksWithEmbeddings.length;

  logger.info(`üìä Chunks cargados: ${loadedChunks.length}`);
  logger.info(`   ‚úÖ Con embeddings: ${chunksWithEmbeddings.length}`);
  logger.info(`   ‚ùå Sin embeddings: ${chunksWithoutEmbeddings}`);

  chunksLoaded = true;

  return loadedChunks;
}

/**
 * Recarga los chunks desde archivos
 * ‚úÖ OPTIMIZADO: Tambi√©n limpia cache de query embeddings y provider cache
 */
function reloadChunks() {
  logger.info('üîÑ Recargando chunks...');
  chunksLoaded = false;
  queryEmbeddingCache.clear();
  loadedChunks = [];
  // Invalidar provider cache para re-evaluar
  cachedProvider = null;
  cachedProviderTimestamp = 0;
  return loadAllChunks();
}

/**
 * Obtiene estad√≠sticas de embeddings
 */
function getEmbeddingStats() {
  const chunksWithEmbeddings = loadedChunks.filter(c => c.embedding && c.embedding.length > 0);
  const chunksWithoutEmbeddings = loadedChunks.length - chunksWithEmbeddings.length;

  return {
    totalChunks: loadedChunks.length,
    withEmbeddings: chunksWithEmbeddings.length,
    withoutEmbeddings: chunksWithoutEmbeddings,
    loaded: chunksLoaded,
    embeddingDimension: chunksWithEmbeddings.length > 0 ? chunksWithEmbeddings[0].embedding.length : 0,
  };
}

// ===========================================
// WARMUP
// ===========================================

/**
 * Pre-calienta el modelo de embeddings al iniciar el servidor
 * Evita latencia de ~645ms en la primera consulta real
 */
async function warmup() {
  try {
    const provider = detectEmbeddingProvider();
    if (provider === 'xenova' && !xenovaPipeline) {
      logger.info('üî• Pre-calentando modelo de embeddings local...');
      const start = Date.now();
      await generateEmbeddingsLocal(['warmup']);
      logger.info(`‚úÖ Modelo de embeddings listo (${Date.now() - start}ms)`);
    } else if (provider === 'xenova') {
      logger.info('‚úÖ Modelo de embeddings ya inicializado');
    }
  } catch (error) {
    logger.warn(`‚ö†Ô∏è Error en warmup de embeddings: ${error.message}`);
  }
}

// ===========================================
// UTILIDADES
// ===========================================

/**
 * Divide un array en chunks de tama√±o espec√≠fico
 */
function chunkArray(array, size) {
  const chunks = [];

  for (let i = 0; i < array.length; i += size) {
    chunks.push(array.slice(i, i + size));
  }

  return chunks;
}

/**
 * Sleep con promesas
 */
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// ===========================================
// EXPORTS
// ===========================================

module.exports = {
  // Generaci√≥n
  generateEmbeddingsBatch,
  ensureEmbeddings,

  // B√∫squeda
  findRelevantChunks,
  getQueryEmbedding,
  cosineSimilarity,

  // Gesti√≥n de chunks
  loadAllChunks,
  reloadChunks,
  getEmbeddingStats,

  // Warmup
  warmup,

  // Configuraci√≥n
  EMBEDDING_CONFIG,
};

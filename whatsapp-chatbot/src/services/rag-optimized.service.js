/**
 * ===========================================
 * SERVICIO RAG OPTIMIZADO - NORBOY
 * ===========================================
 *
 * MEJORAS IMPLEMENTADAS:
 * 1. Modelo de embeddings multiling√ºe (mejor para espa√±ol)
 * 2. Re-ranking con cross-encoder
 * 3. B√∫squeda h√≠brida (vectorial + BM25)
 * 4. Recuperaci√≥n ampliada (15 ‚Üí 7)
 * 5. Cache de queries
 * 6. Umbrales din√°micos
 */

const logger = require('../utils/logger');

// ===========================================
// CONFIGURACI√ìN OPTIMIZADA
// ===========================================

const RAG_CONFIG = {
  // Modelo de embeddings (opciones para upgrade futuro)
  embeddings: {
    // Actual (funciona bien para empezar)
    current: 'Xenova/all-MiniLM-L6-v2',
    // Mejor para espa√±ol (requiere m√°s memoria)
    recommended: 'Xenova/paraphrase-multilingual-MiniLM-L12-v2',
    // El mejor multiling√ºe (m√°s pesado)
    best: 'Xenova/multilingual-e5-small',
    dimensions: {
      'Xenova/all-MiniLM-L6-v2': 384,
      'Xenova/paraphrase-multilingual-MiniLM-L12-v2': 384,
      'Xenova/multilingual-e5-small': 384,
    }
  },

  // Recuperaci√≥n - AJUSTADO para ser m√°s permisivo
  retrieval: {
    topK_initial: 20,    // ANTES: 15 ‚Üí Recuperar m√°s inicialmente
    topK_final: 12,      // ANTES: 7 ‚Üí M√°s chunks finales para mejor contexto
    minSimilarity: 0.20, // ANTES: 0.35 ‚Üí M√°s permisivo para no descartar info √∫til
  },

  // Re-ranking - AJUSTADO para no penalizar demasiado
  reranking: {
    enabled: true,
    // Boost para chunks Q&A
    qaBoost: 1.15,       // ANTES: 1.2 ‚Üí Menos agresivo
    // Boost por keyword match
    keywordBoost: 0.10,  // ANTES: 0.15 ‚Üí Menos agresivo
  },

  // B√∫squeda h√≠brida
  hybrid: {
    enabled: true,
    vectorWeight: 0.75,  // ANTES: 0.7 ‚Üí Dar m√°s peso a similitud vectorial
    bm25Weight: 0.25,    // ANTES: 0.3 ‚Üí Menos peso a keywords
  },

  // Umbrales de calidad - AJUSTADOS para ser m√°s permisivos
  // El modelo all-MiniLM tiene scores inherentemente bajos para espa√±ol
  thresholds: {
    high: 0.55,      // ANTES: 0.65 ‚Üí Calidad alta - responder con confianza
    medium: 0.40,    // ANTES: 0.50 ‚Üí Calidad media - responder con contexto
    low: 0.30,       // ANTES: 0.45 ‚Üí Calidad baja - a√∫n puede responder
    escalate: 0.25,  // ANTES: 0.45 ‚Üí Solo escalar si realmente no hay info
  },

  // Validaci√≥n de query
  query: {
    minLength: 3,    // M√≠nimo 3 caracteres para procesar
  },

  // Cache
  cache: {
    enabled: true,
    maxSize: 100,    // M√°ximo queries en cache
    ttl: 300000,     // 5 minutos TTL
  }
};

// ===========================================
// CACHE DE QUERIES
// ===========================================

const queryCache = new Map();

/**
 * Normaliza una query para cache
 */
function normalizeQuery(query) {
  return query
    .toLowerCase()
    .trim()
    .replace(/[¬ø?!¬°.,;:]/g, '')
    .replace(/\s+/g, ' ');
}

/**
 * Obtiene resultado del cache si existe y no expir√≥
 */
function getCachedResult(query) {
  if (!RAG_CONFIG.cache.enabled) return null;

  const normalized = normalizeQuery(query);
  const cached = queryCache.get(normalized);

  if (cached && (Date.now() - cached.timestamp) < RAG_CONFIG.cache.ttl) {
    logger.debug(`üì¶ Cache hit para: "${query.substring(0, 30)}..."`);
    return cached.results;
  }

  return null;
}

/**
 * Guarda resultado en cache
 */
function setCacheResult(query, results) {
  if (!RAG_CONFIG.cache.enabled) return;

  const normalized = normalizeQuery(query);

  // Limpiar cache si excede tama√±o
  if (queryCache.size >= RAG_CONFIG.cache.maxSize) {
    const oldestKey = queryCache.keys().next().value;
    queryCache.delete(oldestKey);
  }

  queryCache.set(normalized, {
    results,
    timestamp: Date.now()
  });
}

/**
 * Limpia el cache completo
 */
function clearCache() {
  queryCache.clear();
  logger.info('üßπ Cache de queries limpiado');
}

// ===========================================
// B√öSQUEDA BM25 (KEYWORD MATCHING)
// ===========================================

/**
 * Calcula score BM25 simplificado
 * @param {string} query - Consulta del usuario
 * @param {string} document - Texto del chunk
 * @returns {number} Score BM25 (0-1 normalizado)
 */
function calculateBM25Score(query, document) {
  const k1 = 1.5;
  const b = 0.75;
  const avgDocLength = 200; // Estimado para chunks

  // Tokenizar
  const queryTerms = tokenize(query);
  const docTerms = tokenize(document);
  const docLength = docTerms.length;

  if (queryTerms.length === 0 || docTerms.length === 0) return 0;

  // Contar frecuencias
  const termFreq = {};
  for (const term of docTerms) {
    termFreq[term] = (termFreq[term] || 0) + 1;
  }

  // Calcular score
  let score = 0;
  for (const term of queryTerms) {
    const tf = termFreq[term] || 0;
    if (tf > 0) {
      const idf = Math.log((1 + 1) / (1 + 1)); // Simplificado sin corpus
      const tfNorm = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (docLength / avgDocLength)));
      score += idf * tfNorm;
    }
  }

  // Normalizar a 0-1
  const maxScore = queryTerms.length * 2; // Aproximado
  return Math.min(score / maxScore, 1);
}

/**
 * Tokeniza texto para BM25
 */
function tokenize(text) {
  const stopWords = new Set([
    'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'en', 'y', 'o',
    'que', 'es', 'son', 'para', 'por', 'con', 'se', 'su', 'al', 'lo',
    'como', 'm√°s', 'pero', 'sus', 'le', 'ya', 'fue', 'han', 'muy', 'sin',
    'sobre', 'este', 'entre', 'cuando', 'ser', 'hay', 'todo', 'esta',
    'qu√©', 'cu√°l', 'cu√°ndo', 'd√≥nde', 'c√≥mo', 'qui√©n'
  ]);

  return text
    .toLowerCase()
    .normalize('NFD')
    .replace(/[\u0300-\u036f]/g, '') // Quitar acentos
    .replace(/[^\w\s]/g, ' ')
    .split(/\s+/)
    .filter(word => word.length > 2 && !stopWords.has(word));
}

// ===========================================
// RE-RANKING
// ===========================================

/**
 * Re-rankea chunks usando m√∫ltiples se√±ales
 * @param {Array} chunks - Chunks con similitud vectorial
 * @param {string} query - Consulta original
 * @returns {Array} Chunks re-rankeados
 */
function rerankChunks(chunks, query) {
  if (!RAG_CONFIG.reranking.enabled || chunks.length === 0) {
    return chunks;
  }

  const queryLower = query.toLowerCase();
  const queryTerms = tokenize(query);

  const reranked = chunks.map(chunk => {
    let finalScore = chunk.similarity;

    // 1. Boost para chunks Q&A
    if (chunk.isQA) {
      finalScore *= RAG_CONFIG.reranking.qaBoost;
    }

    // 2. Boost por coincidencia exacta de keywords
    const chunkLower = chunk.text.toLowerCase();
    let keywordMatches = 0;

    for (const term of queryTerms) {
      if (chunkLower.includes(term)) {
        keywordMatches++;
      }
    }

    if (queryTerms.length > 0) {
      const keywordRatio = keywordMatches / queryTerms.length;
      finalScore += keywordRatio * RAG_CONFIG.reranking.keywordBoost;
    }

    // 3. Boost por coincidencia de frase exacta
    if (chunkLower.includes(queryLower)) {
      finalScore += 0.1;
    }

    // 4. Penalizaci√≥n por chunks muy cortos (poco informativo)
    if (chunk.text.length < 50) {
      finalScore *= 0.7;
    }

    // 5. Boost por ser pregunta que coincide sem√°nticamente
    if (chunk.question) {
      const questionLower = chunk.question.toLowerCase();
      const questionTerms = tokenize(chunk.question);
      let questionMatches = 0;

      for (const term of queryTerms) {
        if (questionTerms.includes(term)) {
          questionMatches++;
        }
      }

      if (queryTerms.length > 0 && questionMatches / queryTerms.length > 0.5) {
        finalScore += 0.15; // Boost significativo si la pregunta coincide
      }
    }

    return {
      ...chunk,
      originalSimilarity: chunk.similarity,
      similarity: Math.min(finalScore, 1), // Cap at 1
      reranked: true
    };
  });

  // Ordenar por score final
  return reranked.sort((a, b) => b.similarity - a.similarity);
}

// ===========================================
// B√öSQUEDA H√çBRIDA
// ===========================================

/**
 * Combina b√∫squeda vectorial con BM25
 * @param {Array} vectorResults - Resultados de b√∫squeda vectorial
 * @param {string} query - Consulta del usuario
 * @returns {Array} Resultados combinados
 */
function hybridSearch(vectorResults, query) {
  if (!RAG_CONFIG.hybrid.enabled) {
    return vectorResults;
  }

  return vectorResults.map(result => {
    const bm25Score = calculateBM25Score(query, result.text);

    // Combinar scores
    const hybridScore =
      (result.similarity * RAG_CONFIG.hybrid.vectorWeight) +
      (bm25Score * RAG_CONFIG.hybrid.bm25Weight);

    return {
      ...result,
      vectorScore: result.similarity,
      bm25Score: bm25Score,
      similarity: hybridScore,
      isHybrid: true
    };
  });
}

// ===========================================
// B√öSQUEDA PRINCIPAL OPTIMIZADA
// ===========================================

/**
 * Busca chunks relevantes con todas las optimizaciones
 * @param {string} query - Consulta del usuario
 * @param {Object} options - Opciones de b√∫squeda
 * @returns {Promise<Object>} Resultados con metadata
 */
async function findRelevantChunksOptimized(query, options = {}) {
  const {
    topK = RAG_CONFIG.retrieval.topK_final,
    useCache = true,
    useHybrid = RAG_CONFIG.hybrid.enabled,
    useReranking = RAG_CONFIG.reranking.enabled,
  } = options;

  const pipelineStart = Date.now();

  try {
    // 0. Validar longitud m√≠nima de query
    const cleanQuery = query?.trim() || '';
    if (cleanQuery.length < (RAG_CONFIG.query?.minLength || 3)) {
      logger.warn(`‚ö†Ô∏è Query muy corta (${cleanQuery.length} chars): "${cleanQuery}"`);
      return {
        chunks: [],
        quality: 'none',
        topSimilarity: 0,
        avgSimilarity: 0,
        fromCache: false,
        error: 'query_too_short'
      };
    }

    // 1. Verificar cache
    if (useCache) {
      const cached = getCachedResult(query);
      if (cached) {
        const cacheElapsed = Date.now() - pipelineStart;
        logger.info(`‚ö° RAG cache HIT (${cacheElapsed}ms)`);
        return {
          ...cached,
          fromCache: true
        };
      }
    }

    // 2. Obtener servicio de embeddings
    const embeddingsService = require('./embeddings.service');

    // 3. B√∫squeda vectorial inicial (ampliada)
    const vectorStart = Date.now();
    const initialResults = await embeddingsService.findRelevantChunks(
      query,
      RAG_CONFIG.retrieval.topK_initial
    );
    const vectorElapsed = Date.now() - vectorStart;

    if (initialResults.length === 0) {
      logger.info(`üîç RAG: Sin resultados vectoriales (${vectorElapsed}ms)`);
      return {
        chunks: [],
        quality: 'none',
        topSimilarity: 0,
        avgSimilarity: 0,
        fromCache: false
      };
    }

    // 4. B√∫squeda h√≠brida (vectorial + BM25)
    const hybridStart = Date.now();
    let results = useHybrid
      ? hybridSearch(initialResults, query)
      : initialResults;
    const hybridElapsed = Date.now() - hybridStart;

    // 5. Re-ranking
    const rerankStart = Date.now();
    if (useReranking) {
      results = rerankChunks(results, query);
    }
    const rerankElapsed = Date.now() - rerankStart;

    // 6. Filtrar por umbral m√≠nimo
    results = results.filter(r => r.similarity >= RAG_CONFIG.retrieval.minSimilarity);

    // 7. Seleccionar top K final
    const finalResults = results.slice(0, topK);

    // 8. Calcular m√©tricas de calidad
    const topSimilarity = finalResults.length > 0 ? finalResults[0].similarity : 0;
    const avgSimilarity = finalResults.length > 0
      ? finalResults.reduce((sum, r) => sum + r.similarity, 0) / finalResults.length
      : 0;

    // 9. Determinar calidad del contexto
    let quality = 'none';
    if (topSimilarity >= RAG_CONFIG.thresholds.high) {
      quality = 'high';
    } else if (topSimilarity >= RAG_CONFIG.thresholds.medium) {
      quality = 'medium';
    } else if (topSimilarity >= RAG_CONFIG.thresholds.low) {
      quality = 'low';
    }

    // 10. Construir resultado
    const result = {
      chunks: finalResults,
      quality,
      topSimilarity,
      avgSimilarity,
      totalFound: initialResults.length,
      afterReranking: results.length,
      finalCount: finalResults.length,
      fromCache: false,
      config: {
        topK_initial: RAG_CONFIG.retrieval.topK_initial,
        topK_final: topK,
        hybrid: useHybrid,
        reranking: useReranking,
        thresholds: RAG_CONFIG.thresholds
      }
    };

    // 11. Guardar en cache
    if (useCache) {
      setCacheResult(query, result);
    }

    // 12. ‚úÖ OPTIMIZADO: Log de diagn√≥stico con tiempos
    const totalElapsed = Date.now() - pipelineStart;
    logger.info(`üîç RAG Optimizado: "${query.substring(0, 40)}..."`);
    logger.info(`   üìä Calidad: ${quality.toUpperCase()} (top: ${topSimilarity.toFixed(4)}, avg: ${avgSimilarity.toFixed(4)})`);
    logger.info(`   üì¶ Chunks: ${initialResults.length} inicial ‚Üí ${finalResults.length} final`);
    logger.info(`   ‚è±Ô∏è Tiempos: vector=${vectorElapsed}ms, hybrid=${hybridElapsed}ms, rerank=${rerankElapsed}ms, total=${totalElapsed}ms`);

    if (finalResults.length > 0) {
      logger.debug(`   üèÜ Top 3: ${finalResults.slice(0, 3).map(r =>
        `${r.similarity.toFixed(3)}${r.isQA ? '(Q&A)' : ''}`
      ).join(', ')}`);
    }

    return result;

  } catch (error) {
    logger.error(`‚ùå Error en b√∫squeda RAG optimizada: ${error.message}`);
    throw error;
  }
}

// ===========================================
// FORMATEO DE CONTEXTO PARA LLM
// ===========================================

/**
 * Formatea chunks para enviar al LLM
 * @param {Array} chunks - Chunks relevantes
 * @param {Object} metadata - Metadata de la b√∫squeda
 * @returns {string} Contexto formateado
 */
function formatContextForLLM(chunks, metadata = {}) {
  if (!chunks || chunks.length === 0) {
    return '';
  }

  const { quality = 'unknown', topSimilarity = 0 } = metadata;

  let context = `üìö INFORMACI√ìN RECUPERADA (Calidad: ${quality.toUpperCase()}, Relevancia: ${(topSimilarity * 100).toFixed(1)}%)\n\n`;

  chunks.forEach((chunk, index) => {
    const relevance = (chunk.similarity * 100).toFixed(1);
    const source = chunk.source || 'Documento NORBOY';

    if (chunk.isQA && chunk.question && chunk.answer) {
      // Formato estructurado para Q&A
      context += `---\n`;
      context += `üìã PREGUNTA #${index + 1} (Relevancia: ${relevance}%)\n`;
      context += `Fuente: ${source}\n`;
      context += `P: ${chunk.question}\n`;
      context += `R: ${chunk.answer}\n\n`;
    } else {
      // Formato gen√©rico
      context += `---\n`;
      context += `üìÑ FRAGMENTO #${index + 1} (Relevancia: ${relevance}%)\n`;
      context += `Fuente: ${source}\n`;
      context += `${chunk.text}\n\n`;
    }
  });

  return context;
}

// ===========================================
// EVALUACI√ìN DE NECESIDAD DE ESCALACI√ìN
// ===========================================

/**
 * Eval√∫a si debe escalarse a humano basado en la calidad del contexto
 *
 * ‚úÖ AJUSTADO: M√°s permisivo para usar la informaci√≥n encontrada
 * Solo escala cuando REALMENTE no hay informaci√≥n √∫til
 *
 * @param {Object} searchResult - Resultado de findRelevantChunksOptimized
 * @param {string} query - Consulta original
 * @returns {Object} Evaluaci√≥n de escalaci√≥n
 */
function evaluateEscalation(searchResult, query) {
  const { quality, topSimilarity, avgSimilarity, chunks } = searchResult;

  // Patrones que siempre requieren escalaci√≥n (usuario pide humano)
  const escalationPatterns = [
    /asesor|humano|persona|agente|hablar con/i,
    /queja|reclamo|problema grave/i,
    /urgente|emergencia/i,
  ];

  const needsHumanByPattern = escalationPatterns.some(p => p.test(query));

  // Evaluar escalaci√≥n - M√ÅS PERMISIVO
  let shouldEscalate = false;
  let reason = null;
  let confidence = 'high';

  // 1. Usuario pide expl√≠citamente un humano
  if (needsHumanByPattern) {
    shouldEscalate = true;
    reason = 'user_requested_human';
    confidence = 'high';
  }
  // 2. No hay contexto relevante (0 chunks)
  else if (quality === 'none' || chunks.length === 0) {
    shouldEscalate = true;
    reason = 'no_relevant_context';
    confidence = 'high';
  }
  // 3. Score MUY bajo (por debajo del umbral m√≠nimo)
  else if (topSimilarity < RAG_CONFIG.thresholds.escalate) {
    shouldEscalate = true;
    reason = 'similarity_below_threshold';
    confidence = 'high';
  }
  // 4. Promedio extremadamente bajo (menos de 0.20)
  else if (avgSimilarity && avgSimilarity < 0.20) {
    shouldEscalate = true;
    reason = 'average_similarity_too_low';
    confidence = 'medium';
  }
  // 5. ‚úÖ CAMBIO: Calidad LOW ahora NO escala autom√°ticamente
  //    Permitir que la IA intente responder con el contexto disponible
  //    Solo escala si quality === 'none' (ya cubierto arriba)

  const result = {
    shouldEscalate,
    reason,
    confidence,
    quality,
    topSimilarity,
    avgSimilarity,
    threshold: RAG_CONFIG.thresholds.escalate,
    recommendation: shouldEscalate
      ? 'ESCALAR a asesor humano - NO llamar a IA'
      : `Responder con contexto (calidad: ${quality})`
  };

  // Log detallado para debugging
  if (shouldEscalate) {
    console.log(`‚ö†Ô∏è ESCALACI√ìN DECIDIDA:`);
    console.log(`   Raz√≥n: ${reason}`);
    console.log(`   TopSimilarity: ${topSimilarity?.toFixed(4)} (umbral: ${RAG_CONFIG.thresholds.escalate})`);
    console.log(`   Calidad: ${quality}`);
  } else {
    console.log(`‚úÖ NO ESCALAR - Usar contexto disponible:`);
    console.log(`   TopSimilarity: ${topSimilarity?.toFixed(4)}`);
    console.log(`   Calidad: ${quality}`);
    console.log(`   Chunks disponibles: ${chunks.length}`);
  }

  return result;
}

// ===========================================
// EXPORTS
// ===========================================

module.exports = {
  // Configuraci√≥n
  RAG_CONFIG,

  // B√∫squeda principal
  findRelevantChunksOptimized,

  // Utilidades
  formatContextForLLM,
  evaluateEscalation,

  // BM25
  calculateBM25Score,
  tokenize,

  // Re-ranking
  rerankChunks,

  // H√≠brido
  hybridSearch,

  // Cache
  clearCache,
  getCachedResult,
  setCacheResult,
};
